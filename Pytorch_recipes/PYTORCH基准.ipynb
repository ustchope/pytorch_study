{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cd63cb7",
   "metadata": {},
   "source": [
    "本秘籍提供了使用PyTorch基准测试模块测量和比较代码性能的快速入门指南。\n",
    "\n",
    "# 介绍\n",
    "\n",
    "基准测试是写代码的重要步骤。它是验证我们的代码是否满足预期的性能方法，比较解决同一问题的不同并防止回归性能。\n",
    "\n",
    "在对 Pyorch 代码进行基准测试时有很多选择，包括 Python 内置的时间它模块。但是，对 PyTorch 代码进行基准测试有很多容易被注意事项，管理线程数量和同步 CUDA 设备。另外，为基准测试 生成 Tensor 输入可能非常乏味。\n",
    "\n",
    "这个秘籍演示了如何使用 PyTorch 基准测试模块来避免常见错误，同时更容易比较不同代码的性能、生成基准测试输入等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856050e7",
   "metadata": {},
   "source": [
    "# 步骤\n",
    "1. 定义函数以进行基准测试\n",
    "2. 使用 timeit.Timer 进行基准测试\n",
    "3. 使用 torch.utils.benchmark.Timer 进行基准测试\n",
    "4. 使用阻塞式自动量程进行基准测试\n",
    "5. 比较基准结果\n",
    "6. 保存/加载基准测试结果\n",
    "7. 使用模糊参数生成输入\n",
    "8. 使用 Callgrind 收集指令计数\n",
    "\n",
    "1. 定义函数以进行基准测试\n",
    "\n",
    "在撰写本文时，torch.dot 不支持批处理模式，因此我们将比较使用现有 Torch 运算符实现它的两种方法：一种方法使用 mul 和 sum 的组合，而另一种方法将问题减少到 bmm。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b7c716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def batched_dot_mul_sum(a, b):\n",
    "    '''Computes batched dot by multiplying and summing'''\n",
    "    return a.mul(b).sum(-1)\n",
    "\n",
    "\n",
    "def batched_dot_bmm(a, b):\n",
    "    '''Computes batched dot by reducing to bmm'''\n",
    "    a = a.reshape(-1, 1, a.shape[-1])\n",
    "    b = b.reshape(-1, b.shape[-1], 1)\n",
    "    return torch.bmm(a, b).flatten(-3)\n",
    "\n",
    "\n",
    "# Input for benchmarking\n",
    "x = torch.randn(10000, 64)\n",
    "\n",
    "# Ensure that both functions compute the same output\n",
    "assert batched_dot_mul_sum(x, x).allclose(batched_dot_bmm(x, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef12156",
   "metadata": {},
   "source": [
    "2. 使用 timeit.Timer 进行基准测试\n",
    "\n",
    "首先，让我们使用 Python 的内置 timeit 模块对代码进行基准测试。 我们在这里保持基准代码简单，以便我们可以比较 timeit 和 torch.utils.benchmark 的默认值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4901b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul_sum(x, x):  123.0 us\n",
      "bmm(x, x):      122.9 us\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "t0 = timeit.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='from __main__ import batched_dot_mul_sum',\n",
    "    globals={'x': x})\n",
    "\n",
    "t1 = timeit.Timer(\n",
    "    stmt='batched_dot_bmm(x, x)',\n",
    "    setup='from __main__ import batched_dot_bmm',\n",
    "    globals={'x': x})\n",
    "\n",
    "print(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:>5.1f} us')\n",
    "print(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:>5.1f} us')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ea0df",
   "metadata": {},
   "source": [
    "3. 使用 torch.utils.benchmark.Timer 进行基准测试\n",
    "\n",
    "PyTorch 基准测试模块旨在让之前使用过 timeit 模块的人熟悉。 但是，它的默认设置使得用于对 PyTorch 代码进行基准测试更容易、更安全。 让我们首先比较与上面相同的基本 API。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a063e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7ff128c76bb0>\n",
      "batched_dot_mul_sum(x, x)\n",
      "setup: from __main__ import batched_dot_mul_sum\n",
      "  409.42 us\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7ff12d42fca0>\n",
      "batched_dot_bmm(x, x)\n",
      "setup: from __main__ import batched_dot_bmm\n",
      "  764.43 us\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='from __main__ import batched_dot_mul_sum',\n",
    "    globals={'x': x})\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='batched_dot_bmm(x, x)',\n",
    "    setup='from __main__ import batched_dot_bmm',\n",
    "    globals={'x': x})\n",
    "\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1827cf9c",
   "metadata": {},
   "source": [
    "尽管 API 的基本功能相同，但还是存在一些重要差异。 benchmark.Timer.timeit() 返回每次运行的时间，而不是像 timeit.Timer.timeit() 那样的总运行时间。 PyTorch 基准模块还提供用于打印结果的格式化字符串表示。\n",
    "\n",
    "另一个重要的区别，也是结果不同的原因是 PyTorch 基准测试模块默认在单线程中运行。 我们可以使用 num_threads 参数更改线程数。\n",
    "\n",
    "torch.utils.benchmark.Timer 需要几个额外的参数，包括：label、sub_label、description 和 env，它们改变了返回的测量对象的 __repr__ 并用于对结果进行分组（稍后会详细介绍）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd225734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking on 20 threads\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7ff1294ba220>\n",
      "Multithreaded batch dot: Implemented using mul and sum\n",
      "setup: from __main__ import batched_dot_mul_sum\n",
      "  84.94 us\n",
      "  1 measurement, 100 runs , 20 threads\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7ff1294ba700>\n",
      "Multithreaded batch dot: Implemented using bmm\n",
      "setup: from __main__ import batched_dot_bmm\n",
      "  107.02 us\n",
      "  1 measurement, 100 runs , 20 threads\n"
     ]
    }
   ],
   "source": [
    "num_threads = torch.get_num_threads()\n",
    "print(f'Benchmarking on {num_threads} threads')\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='from __main__ import batched_dot_mul_sum',\n",
    "    globals={'x': x},\n",
    "    num_threads=num_threads,\n",
    "    label='Multithreaded batch dot',\n",
    "    sub_label='Implemented using mul and sum')\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='batched_dot_bmm(x, x)',\n",
    "    setup='from __main__ import batched_dot_bmm',\n",
    "    globals={'x': x},\n",
    "    num_threads=num_threads,\n",
    "    label='Multithreaded batch dot',\n",
    "    sub_label='Implemented using bmm')\n",
    "\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266ad628",
   "metadata": {},
   "source": [
    "使用所有可用线程运行基准测试给出与 timeit 模块类似的结果。 更重要的是，哪个版本更快取决于我们运行代码的线程数。 这就是为什么使用代表实际用例的线程设置对代码进行基准测试很重要的原因。 要记住的另一件重要事情是在 GPU 上进行基准测试时同步 CPU 和 CUDA。 让我们在 CUDA 张量上再次运行上述基准测试，看看会发生什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "507aa9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mul_sum(x, x):   40.3 us\n",
      "mul_sum(x, x):   36.2 us\n",
      "bmm(x, x):      985.3 us\n",
      "bmm(x, x):       45.8 us\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10000, 1024, device='cuda:0')\n",
    "\n",
    "t0 = timeit.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='from __main__ import batched_dot_mul_sum',\n",
    "    globals={'x': x})\n",
    "\n",
    "t1 = timeit.Timer(\n",
    "    stmt='batched_dot_bmm(x, x)',\n",
    "    setup='from __main__ import batched_dot_bmm',\n",
    "    globals={'x': x})\n",
    "\n",
    "# Ran each twice to show difference before/after warmup\n",
    "print(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:>5.1f} us')\n",
    "print(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:>5.1f} us')\n",
    "print(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:>5.1f} us')\n",
    "print(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:>5.1f} us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ecbb2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7ff12d42fbb0>\n",
      "batched_dot_mul_sum(x, x)\n",
      "setup: from __main__ import batched_dot_mul_sum\n",
      "  350.22 us\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7ff1294bacd0>\n",
      "batched_dot_bmm(x, x)\n",
      "setup: from __main__ import batched_dot_bmm\n",
      "  346.89 us\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "t0 = benchmark.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='from __main__ import batched_dot_mul_sum',\n",
    "    globals={'x': x})\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='batched_dot_bmm(x, x)',\n",
    "    setup='from __main__ import batched_dot_bmm',\n",
    "    globals={'x': x})\n",
    "\n",
    "# Run only once since benchmark module does warmup for us\n",
    "print(t0.timeit(100))\n",
    "print(t1.timeit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc197ce",
   "metadata": {},
   "source": [
    "结果揭示了一些有趣的事情。 使用 timeit 模块的 bmm 版本的第一次运行比第二次运行要长得多。 这是因为 bmm 调用了 cuBLAS，它需要在第一次调用时加载，这需要一些时间。 这就是为什么在基准测试之前进行热身运行很重要的原因，对我们来说幸运的是，PyTorch 的基准测试模块会处理这个问题。\n",
    "\n",
    "timeit 和 benchmark 模块之间的结果差异是因为 timeit 模块不同步 CUDA，因此只计时启动内核的时间。 PyTorch 的 benchmark 模块为我们做同步。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f079f602",
   "metadata": {},
   "source": [
    "4. 使用阻塞式自动量程进行基准测试\n",
    "\n",
    "虽然 timeit.Timer.autorange 进行至少 0.2 秒的单次连续测量，但 torch.utils.benchmark.blocked_autorange 进行多次测量，其时间总计至少为 0.2 秒（可以通过 min_run_time 参数更改）受时间限制 开销只是整体测量的一小部分。 这是通过首先在每个循环中增加运行次数来实现的，直到运行时间远大于测量开销（这也用作预热），然后进行测量直到达到目标时间。 这具有有用的特性，即浪费更少的数据，并允许我们计算统计数据来估计测量的可靠性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a50810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7ff1294ba1c0>\n",
      "batched_dot_mul_sum(x, x)\n",
      "setup: from __main__ import batched_dot_mul_sum\n",
      "  327.98 us\n",
      "  1 measurement, 1000 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7ff1294baf70>\n",
      "batched_dot_bmm(x, x)\n",
      "setup: from __main__ import batched_dot_bmm\n",
      "  315.80 us\n",
      "  1 measurement, 1000 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "m0 = t0.blocked_autorange()\n",
    "m1 = t1.blocked_autorange()\n",
    "\n",
    "print(m0)\n",
    "print(m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d133282",
   "metadata": {},
   "source": [
    "我们还可以从返回的测量对象中检查单个统计信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03a27f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:   327.98 us\n",
      "Median: 327.98 us\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean:   {m0.mean * 1e6:6.2f} us\")\n",
    "print(f\"Median: {m0.median * 1e6:6.2f} us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1be428",
   "metadata": {},
   "source": [
    "5. 比较基准结果\n",
    "\n",
    "到目前为止，我们一直在将两个版本的批处理点与单个输入进行比较。 在实践中，我们想尝试输入的组合以及不同数量的线程。 Compare 类有助于在格式化的表格中显示许多测量的结果。 它使用上面描述的注释（label、sub_label、num_threads 等）以及 description 来对表进行分组和组织。 让我们使用 Compare 来看看我们的函数在不同的输入大小和线程数下的表现如何。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52af92ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------- Batched dot ----------------]\n",
      "                      |  mul/sum   |    bmm   \n",
      "1 threads: -----------------------------------\n",
      "      [1, 1]          |       8.5  |      12.6\n",
      "      [1, 64]         |       4.8  |       7.5\n",
      "      [1, 1024]       |       5.1  |       8.5\n",
      "      [1, 10000]      |       6.7  |       9.3\n",
      "      [64, 1]         |       4.9  |       8.4\n",
      "      [64, 64]        |       6.7  |      12.7\n",
      "      [64, 1024]      |      26.8  |     164.6\n",
      "      [64, 10000]     |     341.5  |    1478.8\n",
      "      [1024, 1]       |       6.1  |      14.9\n",
      "      [1024, 64]      |      34.7  |      86.4\n",
      "      [1024, 1024]    |     531.7  |    2420.1\n",
      "      [1024, 10000]   |   23665.7  |   23536.0\n",
      "      [10000, 1]      |      13.6  |      74.9\n",
      "      [10000, 64]     |     388.0  |     722.1\n",
      "      [10000, 1024]   |   23451.2  |   23552.1\n",
      "      [10000, 10000]  |  238468.1  |  228073.2\n",
      "4 threads: -----------------------------------\n",
      "      [1, 1]          |       8.5  |      12.6\n",
      "      [1, 64]         |       4.8  |       7.5\n",
      "      [1, 1024]       |       5.2  |       8.4\n",
      "      [1, 10000]      |       6.6  |       9.3\n",
      "      [64, 1]         |       4.9  |      13.1\n",
      "      [64, 64]        |       6.5  |      14.0\n",
      "      [64, 1024]      |      50.5  |     273.2\n",
      "      [64, 10000]     |      70.0  |    3941.7\n",
      "      [1024, 1]       |       6.1  |      15.5\n",
      "      [1024, 64]      |      30.7  |      37.9\n",
      "      [1024, 1024]    |     147.0  |     742.0\n",
      "      [1024, 10000]   |    9886.5  |    6517.3\n",
      "      [10000, 1]      |      13.6  |      29.0\n",
      "      [10000, 64]     |      89.8  |     200.2\n",
      "      [10000, 1024]   |    9997.6  |    6562.5\n",
      "      [10000, 10000]  |  106601.8  |   62611.7\n",
      "16 threads: ----------------------------------\n",
      "      [1, 1]          |       8.6  |      12.5\n",
      "      [1, 64]         |       4.8  |       7.6\n",
      "      [1, 1024]       |       5.2  |       8.4\n",
      "      [1, 10000]      |       6.7  |       9.3\n",
      "      [64, 1]         |       8.8  |      28.6\n",
      "      [64, 64]        |       6.5  |      15.8\n",
      "      [64, 1024]      |      40.3  |     607.1\n",
      "      [64, 10000]     |      40.3  |    5554.6\n",
      "      [1024, 1]       |       6.1  |      16.1\n",
      "      [1024, 64]      |      42.1  |      28.3\n",
      "      [1024, 1024]    |      63.5  |     254.3\n",
      "      [1024, 10000]   |    8992.3  |    2341.6\n",
      "      [10000, 1]      |      13.6  |      23.2\n",
      "      [10000, 64]     |      59.5  |      97.4\n",
      "      [10000, 1024]   |    9167.3  |    2082.6\n",
      "      [10000, 10000]  |   89199.6  |   22880.7\n",
      "32 threads: ----------------------------------\n",
      "      [1, 1]          |       8.5  |       7.5\n",
      "      [1, 64]         |       4.8  |       7.6\n",
      "      [1, 1024]       |       5.2  |       8.4\n",
      "      [1, 10000]      |       6.6  |       9.4\n",
      "      [64, 1]         |       8.8  |      24.5\n",
      "      [64, 64]        |       6.7  |      35.5\n",
      "      [64, 1024]      |      74.9  |     541.6\n",
      "      [64, 10000]     |     212.9  |    4899.2\n",
      "      [1024, 1]       |       6.3  |      24.9\n",
      "      [1024, 64]      |      60.1  |      29.9\n",
      "      [1024, 1024]    |      79.9  |     393.8\n",
      "      [1024, 10000]   |    8035.5  |    2272.3\n",
      "      [10000, 1]      |      13.6  |      25.2\n",
      "      [10000, 64]     |     226.4  |      64.0\n",
      "      [10000, 1024]   |    8457.3  |    1581.0\n",
      "      [10000, 10000]  |   86718.9  |   23550.3\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Compare takes a list of measurements which we'll save in results.\n",
    "results = []\n",
    "\n",
    "sizes = [1, 64, 1024, 10000]\n",
    "for b, n in product(sizes, sizes):\n",
    "    # label and sub_label are the rows\n",
    "    # description is the column\n",
    "    label = 'Batched dot'\n",
    "    sub_label = f'[{b}, {n}]'\n",
    "    x = torch.ones((b, n))\n",
    "    for num_threads in [1, 4, 16, 32]:\n",
    "        results.append(benchmark.Timer(\n",
    "            stmt='batched_dot_mul_sum(x, x)',\n",
    "            setup='from __main__ import batched_dot_mul_sum',\n",
    "            globals={'x': x},\n",
    "            num_threads=num_threads,\n",
    "            label=label,\n",
    "            sub_label=sub_label,\n",
    "            description='mul/sum',\n",
    "        ).blocked_autorange(min_run_time=1))\n",
    "        results.append(benchmark.Timer(\n",
    "            stmt='batched_dot_bmm(x, x)',\n",
    "            setup='from __main__ import batched_dot_bmm',\n",
    "            globals={'x': x},\n",
    "            num_threads=num_threads,\n",
    "            label=label,\n",
    "            sub_label=sub_label,\n",
    "            description='bmm',\n",
    "        ).blocked_autorange(min_run_time=1))\n",
    "\n",
    "compare = benchmark.Compare(results)\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d5a5da",
   "metadata": {},
   "source": [
    "上面的结果表明，对于在多线程上运行的较大张量，减少到 bmm 的版本更好，而对于较小和/或单线程代码，另一个版本更好。\n",
    "\n",
    "比较还提供了改变表格格式的功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "959dd5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-------------- Batched dot --------------]\n",
      "                      |  mul/sum  |   bmm  \n",
      "1 threads: --------------------------------\n",
      "      [1, 1]          |        8  |      13\n",
      "      [1, 64]         |  \u001b[92m\u001b[1m      5\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m     8\u001b[0m\u001b[0m\n",
      "      [1, 1024]       |  \u001b[34m\u001b[1m      5\u001b[0m\u001b[0m  |       8\n",
      "      [1, 10000]      |        7  |       9\n",
      "      [64, 1]         |  \u001b[34m\u001b[1m      5\u001b[0m\u001b[0m  |       8\n",
      "      [64, 64]        |        7  |      13\n",
      "      [64, 1024]      |  \u001b[31m\u001b[1m     27\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   160\u001b[0m\u001b[0m\n",
      "      [64, 10000]     |  \u001b[31m\u001b[1m    340\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  1500\u001b[0m\u001b[0m\n",
      "      [1024, 1]       |        6  |      15\n",
      "      [1024, 64]      |  \u001b[31m\u001b[1m     35\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m    86\u001b[0m\u001b[0m\n",
      "      [1024, 1024]    |  \u001b[31m\u001b[1m    532\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  2400\u001b[0m\u001b[0m\n",
      "      [1024, 10000]   |  \u001b[31m\u001b[1m  24000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 24000\u001b[0m\u001b[0m\n",
      "      [10000, 1]      |  \u001b[2m\u001b[91m     14\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m    75\u001b[0m\u001b[0m\n",
      "      [10000, 64]     |  \u001b[31m\u001b[1m    390\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   720\u001b[0m\u001b[0m\n",
      "      [10000, 1024]   |  \u001b[31m\u001b[1m  23000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 23600\u001b[0m\u001b[0m\n",
      "      [10000, 10000]  |  \u001b[31m\u001b[1m 200000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m230000\u001b[0m\u001b[0m\n",
      "4 threads: --------------------------------\n",
      "      [1, 1]          |        8  |      13\n",
      "      [1, 64]         |  \u001b[92m\u001b[1m      5\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m     8\u001b[0m\u001b[0m\n",
      "      [1, 1024]       |  \u001b[34m\u001b[1m      5\u001b[0m\u001b[0m  |       8\n",
      "      [1, 10000]      |        7  |       9\n",
      "      [64, 1]         |  \u001b[34m\u001b[1m      5\u001b[0m\u001b[0m  |      13\n",
      "      [64, 64]        |        6  |      10\n",
      "      [64, 1024]      |  \u001b[31m\u001b[1m     51\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   273\u001b[0m\u001b[0m\n",
      "      [64, 10000]     |  \u001b[31m\u001b[1m     70\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  4000\u001b[0m\u001b[0m\n",
      "      [1024, 1]       |        6  |  \u001b[2m\u001b[91m    16\u001b[0m\u001b[0m\n",
      "      [1024, 64]      |  \u001b[31m\u001b[1m     31\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m    40\u001b[0m\u001b[0m\n",
      "      [1024, 1024]    |  \u001b[31m\u001b[1m    150\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   700\u001b[0m\u001b[0m\n",
      "      [1024, 10000]   |  \u001b[31m\u001b[1m   9890\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  7000\u001b[0m\u001b[0m\n",
      "      [10000, 1]      |  \u001b[2m\u001b[91m     14\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m    30\u001b[0m\u001b[0m\n",
      "      [10000, 64]     |  \u001b[31m\u001b[1m     90\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   200\u001b[0m\u001b[0m\n",
      "      [10000, 1024]   |  \u001b[31m\u001b[1m  10000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  6560\u001b[0m\u001b[0m\n",
      "      [10000, 10000]  |  \u001b[31m\u001b[1m 110000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 60000\u001b[0m\u001b[0m\n",
      "16 threads: -------------------------------\n",
      "      [1, 1]          |        9  |      12\n",
      "      [1, 64]         |  \u001b[92m\u001b[1m      5\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m     8\u001b[0m\u001b[0m\n",
      "      [1, 1024]       |  \u001b[34m\u001b[1m      5\u001b[0m\u001b[0m  |       8\n",
      "      [1, 10000]      |        7  |       9\n",
      "      [64, 1]         |        9  |  \u001b[2m\u001b[91m    29\u001b[0m\u001b[0m\n",
      "      [64, 64]        |        6  |  \u001b[2m\u001b[91m    16\u001b[0m\u001b[0m\n",
      "      [64, 1024]      |  \u001b[31m\u001b[1m     40\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   610\u001b[0m\u001b[0m\n",
      "      [64, 10000]     |  \u001b[31m\u001b[1m     40\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  5555\u001b[0m\u001b[0m\n",
      "      [1024, 1]       |        6  |  \u001b[2m\u001b[91m    16\u001b[0m\u001b[0m\n",
      "      [1024, 64]      |  \u001b[31m\u001b[1m     42\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m    28\u001b[0m\u001b[0m\n",
      "      [1024, 1024]    |  \u001b[31m\u001b[1m     64\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   300\u001b[0m\u001b[0m\n",
      "      [1024, 10000]   |  \u001b[31m\u001b[1m   9000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  2000\u001b[0m\u001b[0m\n",
      "      [10000, 1]      |  \u001b[2m\u001b[91m     14\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m    23\u001b[0m\u001b[0m\n",
      "      [10000, 64]     |  \u001b[31m\u001b[1m     60\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m    97\u001b[0m\u001b[0m\n",
      "      [10000, 1024]   |  \u001b[31m\u001b[1m   9200\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  2000\u001b[0m\u001b[0m\n",
      "      [10000, 10000]  |  \u001b[31m\u001b[1m  89000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 20000\u001b[0m\u001b[0m\n",
      "32 threads: -------------------------------\n",
      "      [1, 1]          |        8  |  \u001b[92m\u001b[1m     8\u001b[0m\u001b[0m\n",
      "      [1, 64]         |  \u001b[92m\u001b[1m      5\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m     8\u001b[0m\u001b[0m\n",
      "      [1, 1024]       |  \u001b[34m\u001b[1m      5\u001b[0m\u001b[0m  |       8\n",
      "      [1, 10000]      |        7  |       9\n",
      "      [64, 1]         |        9  |  \u001b[2m\u001b[91m    24\u001b[0m\u001b[0m\n",
      "      [64, 64]        |        7  |  \u001b[2m\u001b[91m    40\u001b[0m\u001b[0m\n",
      "      [64, 1024]      |  \u001b[31m\u001b[1m     75\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   500\u001b[0m\u001b[0m\n",
      "      [64, 10000]     |  \u001b[31m\u001b[1m    210\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  5000\u001b[0m\u001b[0m\n",
      "      [1024, 1]       |        6  |  \u001b[2m\u001b[91m    25\u001b[0m\u001b[0m\n",
      "      [1024, 64]      |  \u001b[31m\u001b[1m     60\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m    30\u001b[0m\u001b[0m\n",
      "      [1024, 1024]    |  \u001b[31m\u001b[1m     80\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m   400\u001b[0m\u001b[0m\n",
      "      [1024, 10000]   |  \u001b[31m\u001b[1m   8000\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  2000\u001b[0m\u001b[0m\n",
      "      [10000, 1]      |  \u001b[2m\u001b[91m     14\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m    25\u001b[0m\u001b[0m\n",
      "      [10000, 64]     |  \u001b[31m\u001b[1m    200\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m    60\u001b[0m\u001b[0m\n",
      "      [10000, 1024]   |  \u001b[31m\u001b[1m   8460\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m  1600\u001b[0m\u001b[0m\n",
      "      [10000, 10000]  |  \u001b[31m\u001b[1m  86700\u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m 20000\u001b[0m\u001b[0m\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare.trim_significant_figures()\n",
    "compare.colorize()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfeed10",
   "metadata": {},
   "source": [
    "6. 保存/加载基准测试结果\n",
    "\n",
    "测量值（和第 8 节中描述的 CallgrindStats）是可以选择的。 这使得 A/B 测试变得容易，因为您可以从两个不同的环境中收集测量值，对它们进行腌制，然后将它们加载到单个环境中。 Timer 甚至采用 env 构造函数参数，以便此类 A/B 测试无缝工作。\n",
    "\n",
    "让我们想象一下，不是两个 Python 函数，而是 add/sum 和 bmm 方法在 PyTorch 的两个不同版本中。 下面的示例演示了如何对它们进行 A/B 测试。 为简单起见，我们只使用形状的子集，并通过 pickle 简单地往返结果，而不是实际使用多个环境并将结果写入磁盘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e77a3749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------- Batched dot -------------------------------------]\n",
      "                                               |  [1, 1]  |  [1024, 10000]  |  [10000, 1]\n",
      "1 threads: ------------------------------------------------------------------------------\n",
      "  (environment A: mul/sum)  batched_dot(x, x)  |  \u001b[92m\u001b[1m 4.7  \u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m    24000    \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m    14    \u001b[0m\u001b[0m\n",
      "  (environment B: bmm)      batched_dot(x, x)  |   7.4    |  \u001b[92m\u001b[1m    24000    \u001b[0m\u001b[0m  |  \u001b[31m\u001b[1m    75    \u001b[0m\u001b[0m\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "ab_test_results = []\n",
    "for env in ('environment A: mul/sum', 'environment B: bmm'):\n",
    "    for b, n in ((1, 1), (1024, 10000), (10000, 1)):\n",
    "        x = torch.ones((b, n))\n",
    "        dot_fn = (batched_dot_mul_sum if env == 'environment A: mul/sum' else batched_dot_bmm)\n",
    "        m = benchmark.Timer(\n",
    "            stmt='batched_dot(x, x)',\n",
    "            globals={'x': x, 'batched_dot': dot_fn},\n",
    "            num_threads=1,\n",
    "            label='Batched dot',\n",
    "            description=f'[{b}, {n}]',\n",
    "            env=env,\n",
    "        ).blocked_autorange(min_run_time=1)\n",
    "        ab_test_results.append(pickle.dumps(m))\n",
    "\n",
    "ab_results = [pickle.loads(i) for i in ab_test_results]\n",
    "compare = benchmark.Compare(ab_results)\n",
    "compare.trim_significant_figures()\n",
    "compare.colorize()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3bc26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And just to show that we can round trip all of the results from earlier:\n",
    "round_tripped_results = pickle.loads(pickle.dumps(results))\n",
    "assert(str(benchmark.Compare(results)) == str(benchmark.Compare(round_tripped_results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c07f3",
   "metadata": {},
   "source": [
    "7. 使用模糊参数生成输入\n",
    "\n",
    "正如我们在上一节中看到的，根据输入张量的不同，可能会有一些明显的性能差异。 因此，在许多不同的输入上运行基准测试是一个好主意。 但是，创建所有这些输入张量可能很乏味，这就是 torch.utils.benchmark.Fuzzer 和相关类的用武之地。让我们看看如何使用 Fuzzer 为基准测试创建一些测试用例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf4fd795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------- Batched dot ---------------------]\n",
      "                                     |  mul/sum  |   bmm \n",
      "1 threads: ----------------------------------------------\n",
      "      725    x 257                   |     100   |    248\n",
      "      49     x 383                   |      15   |     38\n",
      "      34     x 1468                  |      19   |    130\n",
      "      187    x 5039                  |     550   |   2200\n",
      "      2140   x 1296 (discontiguous)  |    2000   |  30100\n",
      "      78     x 1598                  |      46   |    304\n",
      "      519    x 763                   |     240   |    956\n",
      "      141    x 1082                  |      67   |    380\n",
      "      78     x 5    (discontiguous)  |      10   |     15\n",
      "      187    x 1                     |       9   |     15\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.benchmark import Fuzzer, FuzzedParameter, FuzzedTensor, ParameterAlias\n",
    "\n",
    "# Generates random tensors with 128 to 10000000 elements and sizes k0 and k1 chosen from a\n",
    "# loguniform distribution in [1, 10000], 40% of which will be discontiguous on average.\n",
    "example_fuzzer = Fuzzer(\n",
    "    parameters = [\n",
    "        FuzzedParameter('k0', minval=1, maxval=10000, distribution='loguniform'),\n",
    "        FuzzedParameter('k1', minval=1, maxval=10000, distribution='loguniform'),\n",
    "    ],\n",
    "    tensors = [\n",
    "        FuzzedTensor('x', size=('k0', 'k1'), min_elements=128, max_elements=10000000, probability_contiguous=0.6)\n",
    "    ],\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "results = []\n",
    "for tensors, tensor_params, params in example_fuzzer.take(10):\n",
    "    # description is the column label\n",
    "    sub_label=f\"{params['k0']:<6} x {params['k1']:<4} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n",
    "    results.append(benchmark.Timer(\n",
    "        stmt='batched_dot_mul_sum(x, x)',\n",
    "        setup='from __main__ import batched_dot_mul_sum',\n",
    "        globals=tensors,\n",
    "        label='Batched dot',\n",
    "        sub_label=sub_label,\n",
    "        description='mul/sum',\n",
    "    ).blocked_autorange(min_run_time=1))\n",
    "    results.append(benchmark.Timer(\n",
    "        stmt='batched_dot_bmm(x, x)',\n",
    "        setup='from __main__ import batched_dot_bmm',\n",
    "        globals=tensors,\n",
    "        label='Batched dot',\n",
    "        sub_label=sub_label,\n",
    "        description='bmm',\n",
    "    ).blocked_autorange(min_run_time=1))\n",
    "\n",
    "compare = benchmark.Compare(results)\n",
    "compare.trim_significant_figures()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0245164b",
   "metadata": {},
   "source": [
    "定义自己的 Fuzzer 有很大的灵活性，这对于创建一组强大的基准测试输入非常有用。 但为了让事情变得更简单，PyTorch 基准测试模块附带了一些用于常见基准测试需求的模糊器。 让我们来看看如何使用这些内置模糊器之一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60c0298d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[----------------------- Batched dot ------------------------]\n",
      "                                         |  mul/sum  |   bmm  \n",
      "1 threads: ---------------------------------------------------\n",
      "      64     x 473  (discontiguous)      |  \u001b[92m\u001b[1m 26500 \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 96000\u001b[0m\u001b[0m\n",
      "      16384  x 12642115 (discontiguous)  |  \u001b[92m\u001b[1m    23 \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m    86\u001b[0m\u001b[0m\n",
      "      8192   x 892                       |  \u001b[92m\u001b[1m  6800 \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m 17100\u001b[0m\u001b[0m\n",
      "      512    x 64   (discontiguous)      |  \u001b[92m\u001b[1m 92000 \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m300000\u001b[0m\u001b[0m\n",
      "      493    x 27   (discontiguous)      |  \u001b[92m\u001b[1m  2300 \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m  4900\u001b[0m\u001b[0m\n",
      "      118    x 32   (discontiguous)      |  \u001b[92m\u001b[1m  1050 \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m  3500\u001b[0m\u001b[0m\n",
      "      16     x 495  (discontiguous)      |  \u001b[92m\u001b[1m 24000 \u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m 25100\u001b[0m\u001b[0m\n",
      "      488    x 62374                     |  \u001b[92m\u001b[1m 70000 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m 70000\u001b[0m\u001b[0m\n",
      "      240372 x 69                        |   40000   |  \u001b[92m\u001b[1m 19000\u001b[0m\u001b[0m\n",
      "      40156  x 32   (discontiguous)      |  \u001b[92m\u001b[1m  2100 \u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m  4600\u001b[0m\u001b[0m\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.benchmark.op_fuzzers import binary\n",
    "\n",
    "results = []\n",
    "for tensors, tensor_params, params in binary.BinaryOpFuzzer(seed=0).take(10):\n",
    "    sub_label=f\"{params['k0']:<6} x {params['k1']:<4} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n",
    "    results.append(benchmark.Timer(\n",
    "        stmt='batched_dot_mul_sum(x, x)',\n",
    "        setup='from __main__ import batched_dot_mul_sum',\n",
    "        globals=tensors,\n",
    "        label='Batched dot',\n",
    "        sub_label=sub_label,\n",
    "        description='mul/sum',\n",
    "    ).blocked_autorange(min_run_time=1))\n",
    "    results.append(benchmark.Timer(\n",
    "        stmt='batched_dot_bmm(x, x)',\n",
    "        setup='from __main__ import batched_dot_bmm',\n",
    "        globals=tensors,\n",
    "        label='Batched dot',\n",
    "        sub_label=sub_label,\n",
    "        description='bmm',\n",
    "    ).blocked_autorange(min_run_time=1))\n",
    "\n",
    "compare = benchmark.Compare(results)\n",
    "compare.trim_significant_figures()\n",
    "compare.colorize(rowwise=True)\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5681b022",
   "metadata": {},
   "source": [
    "8. 使用 Callgrind 收集指令计数\n",
    "\n",
    "优化代码的挑战之一是挂墙时间的变化和不透明度。不确定性的来源有很多，从自适应时钟速度到与其他进程的资源争用。此外，端到端时间无法洞察时间花在哪里，而这正是我们在优化代码时真正感兴趣的。\n",
    "\n",
    "一种补充方法是还收集指令计数。这些计数是一个代理指标，并没有捕获性能的所有方面（例如内存或 I/O 绑定任务），但它们确实有几个有用的属性。指令计数是可重复的，对环境变化不敏感，并提供对程序在哪里花费周期的细粒度洞察。\n",
    "\n",
    "要了解指令计数的效用，让我们看看如何减少 batched_dot_mul_sum 的开销。显而易见的解决方案是将其移至 C++，因此我们避免在 Python 和 C++ 之间多次切换。\n",
    "\n",
    "幸运的是，来源几乎相同。我们在 C++ 中必须问的一个问题是我们应该按值还是按引用来获取参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "761f8f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7feec23c97c0>\n",
      "batched_dot_mul_sum(x, x)\n",
      "setup:\n",
      "  from __main__ import batched_dot_mul_sum\n",
      "  x = torch.randn(2, 2)\n",
      "\n",
      "  4.74 us\n",
      "  1 measurement, 100000 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7ff0193fd9a0>\n",
      "cpp_lib.batched_dot_mul_sum_v0(x, x)\n",
      "setup:\n",
      "  import cpp_lib\n",
      "  x = torch.randn(2, 2)\n",
      "\n",
      "  3.89 us\n",
      "  1 measurement, 100000 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7feec23c97c0>\n",
      "cpp_lib.batched_dot_mul_sum_v1(x, x)\n",
      "setup:\n",
      "  import cpp_lib\n",
      "  x = torch.randn(2, 2)\n",
      "\n",
      "  3.91 us\n",
      "  1 measurement, 100000 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "batched_dot_src = \"\"\"\\\n",
    "/* ---- Python ---- */\n",
    "// def batched_dot_mul_sum(a, b):\n",
    "//     return a.mul(b).sum(-1)\n",
    "\n",
    "torch::Tensor batched_dot_mul_sum_v0(\n",
    "    const torch::Tensor a,\n",
    "    const torch::Tensor b) {\n",
    "  return a.mul(b).sum(-1);\n",
    "}\n",
    "\n",
    "torch::Tensor batched_dot_mul_sum_v1(\n",
    "    const torch::Tensor& a,\n",
    "    const torch::Tensor& b) {\n",
    "  return a.mul(b).sum(-1);\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# PyTorch makes it easy to test our C++ implementations by providing a utility\n",
    "# to JIT compile C++ source into Python extensions:\n",
    "import os\n",
    "from torch.utils import cpp_extension\n",
    "cpp_lib = cpp_extension.load_inline(\n",
    "    name='cpp_lib',\n",
    "    cpp_sources=batched_dot_src,\n",
    "    extra_cflags=['-O3'],\n",
    "    extra_include_paths=[\n",
    "        # `load_inline` needs to know where to find Pybind11 headers.\n",
    "        os.path.join(os.getenv('CONDA_PREFIX'), 'include')\n",
    "    ],\n",
    "    functions=['batched_dot_mul_sum_v0', 'batched_dot_mul_sum_v1']\n",
    ")\n",
    "\n",
    "# `load_inline` will create a shared object that is loaded into Python. When we collect\n",
    "# instruction counts Timer will create a subprocess, so we need to re-import it. The\n",
    "# import process is slightly more complicated for C extensions, but that's all we're\n",
    "# doing here.\n",
    "module_import_str = f\"\"\"\\\n",
    "# https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"cpp_lib\", {repr(cpp_lib.__file__)})\n",
    "cpp_lib = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(cpp_lib)\"\"\"\n",
    "\n",
    "import textwrap\n",
    "def pretty_print(result):\n",
    "    \"\"\"Import machinery for cpp_lib.so can get repetitive to look at.\"\"\"\n",
    "    print(repr(result).replace(textwrap.indent(module_import_str, \"  \"), \"  import cpp_lib\"))\n",
    "\n",
    "\n",
    "t_baseline = benchmark.Timer(\n",
    "    stmt='batched_dot_mul_sum(x, x)',\n",
    "    setup='''\\\n",
    "from __main__ import batched_dot_mul_sum\n",
    "x = torch.randn(2, 2)''')\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt='cpp_lib.batched_dot_mul_sum_v0(x, x)',\n",
    "    setup=f'''\\\n",
    "{module_import_str}\n",
    "x = torch.randn(2, 2)''')\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='cpp_lib.batched_dot_mul_sum_v1(x, x)',\n",
    "    setup=f'''\\\n",
    "{module_import_str}\n",
    "x = torch.randn(2, 2)''')\n",
    "\n",
    "# Moving to C++ did indeed reduce overhead, but it's hard to tell which\n",
    "# calling convention is more efficient. v1 (call with references) seems to\n",
    "# be a bit faster, but it's within measurement error.\n",
    "pretty_print(t_baseline.blocked_autorange())\n",
    "pretty_print(t0.blocked_autorange())\n",
    "pretty_print(t1.blocked_autorange())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b9bd4b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Missing: valgrind, callgrind_control, callgrind_annotate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2902d8545fd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's use Callgrind to determine which is better.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstats_v0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_callgrind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstats_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_callgrind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats_v0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/miniconda3/envs/torch19/lib/python3.9/site-packages/torch/utils/benchmark/utils/timer.py\u001b[0m in \u001b[0;36mcollect_callgrind\u001b[0;34m(self, number, repeats, collect_baseline, retain_out_file)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0mis_python\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_language\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPYTHON\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mis_python\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_globals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         result = valgrind_timer_interface.wrapper_singleton().collect_callgrind(\n\u001b[0m\u001b[1;32m    483\u001b[0m             \u001b[0mtask_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mglobals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_globals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/miniconda3/envs/torch19/lib/python3.9/site-packages/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\u001b[0m in \u001b[0;36mcollect_callgrind\u001b[0;34m(self, task_spec, globals, number, repeats, collect_baseline, is_python, retain_out_file)\u001b[0m\n\u001b[1;32m    524\u001b[0m     ) -> Tuple[CallgrindStats, ...]:\n\u001b[1;32m    525\u001b[0m         \u001b[0;34m\"\"\"Collect stats, and attach a reference run which can be used to filter interpreter overhead.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mis_python\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcollect_baseline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/miniconda3/envs/torch19/lib/python3.9/site-packages/torch/utils/benchmark/utils/valgrind_wrapper/timer_interface.py\u001b[0m in \u001b[0;36m_validate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mmissing_cmds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcmd\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavailable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_commands_available\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mavailable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing_cmds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Missing: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_cmds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     def collect_callgrind(\n",
      "\u001b[0;31mOSError\u001b[0m: Missing: valgrind, callgrind_control, callgrind_annotate"
     ]
    }
   ],
   "source": [
    "# Let's use Callgrind to determine which is better.\n",
    "stats_v0 = t0.collect_callgrind()\n",
    "stats_v1 = t1.collect_callgrind()\n",
    "\n",
    "pretty_print(stats_v0)\n",
    "pretty_print(stats_v1)\n",
    "\n",
    "# `.as_standardized` removes file names and some path prefixes, and makes\n",
    "# it easier to read the function symbols.\n",
    "stats_v0 = stats_v0.as_standardized()\n",
    "stats_v1 = stats_v1.as_standardized()\n",
    "\n",
    "# `.delta` diffs the instruction counts, and `.denoise` removes several\n",
    "# functions in the Python interpreter that are known to have significant\n",
    "# jitter.\n",
    "delta = stats_v1.delta(stats_v0).denoise()\n",
    "\n",
    "# `.transform` is a convenience API for transforming function names. It is\n",
    "# useful for increasing cancelation when diff-ing instructions, as well as\n",
    "# just generally improving readability.\n",
    "replacements = (\n",
    "    (\"???:void pybind11\", \"pybind11\"),\n",
    "    (\"batched_dot_mul_sum_v0\", \"batched_dot_mul_sum_v1\"),\n",
    "    (\"at::Tensor, at::Tensor\", \"...\"),\n",
    "    (\"at::Tensor const&, at::Tensor const&\", \"...\"),\n",
    "    (\"auto torch::detail::wrap_pybind_function_impl_\", \"wrap_pybind_function_impl_\"),\n",
    ")\n",
    "for before, after in replacements:\n",
    "    delta = delta.transform(lambda l: l.replace(before, after))\n",
    "\n",
    "# We can use print options to control how much of the function to display.\n",
    "torch.set_printoptions(linewidth=160)\n",
    "\n",
    "# Once parsed, the instruction counts make clear that passing `a` and `b`\n",
    "# by reference is more efficient as it skips some c10::TensorImpl bookkeeping\n",
    "# for the intermediate Tensors, and is also works better with PyBind11. This\n",
    "# is consistent with our noisy wall time observations.\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835b0972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch19]",
   "language": "python",
   "name": "conda-env-torch19-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
