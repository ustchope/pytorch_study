{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a88866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 280 ms (started: 2021-09-05 15:30:03 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# 自动计算cell的计算时间\n",
    "%load_ext autotime\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='svg' #矢量图设置，让绘图更清晰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74944fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# 增加更新\n",
    "git add *.ipynb */*.ipynb\n",
    "\n",
    "git remote -v\n",
    "\n",
    "git commit -m '更新 #3 Sept 05, 2021'\n",
    "\n",
    "#git push origin master\n",
    "git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d723dc",
   "metadata": {},
   "source": [
    "在构建神经网络时将梯度归零是有益的。这是因为默认情况下，每当调用 .backward() 时，梯度都会在缓冲区中累积（即不会被覆盖）。\n",
    "\n",
    "# 介绍\n",
    "\n",
    "在训练神经网络时，模型能够通过梯度下降来提高其准确性。简而言之，梯度下降是通过调整模型中的权重和偏差来最小化损失（或错误）的过程。\n",
    "\n",
    "torch.Tensor 是 PyTorch 的核心类。创建张量时，如果将其属性 .requires_grad 设置为 True，则该包会跟踪对其进行的所有操作。这发生在随后的向后传递中。此张量的梯度将累积到 .grad 属性中。当在损失张量上调用 .backward() 时，计算所有梯度的累积（或总和）。\n",
    "\n",
    "在某些情况下，可能需要将张量的梯度归零。例如：当您开始训练循环时，您应该将梯度归零，以便您可以正确执行此跟踪。在这个秘籍中，我们将学习如何使用 PyTorch 库将梯度归零。我们将通过在 PyTorch 内置的 CIFAR10 数据集上训练神经网络来演示如何做到这一点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64038c20",
   "metadata": {},
   "source": [
    "# 步骤\n",
    "\n",
    "步骤 1 到 4 设置我们的数据和神经网络以进行训练。 将梯度归零的过程发生在第 5 步。如果您已经构建了数据和神经网络，请跳至第 5 步。\n",
    "* 导入所有必要的库以加载我们的数据\n",
    "* 加载和规范化数据集\n",
    "* 构建神经网络\n",
    "* 定义损失函数\n",
    "* 训练网络时将梯度归零"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebef48e",
   "metadata": {},
   "source": [
    "1. 导入必要的库来加载我们的数据\n",
    "\n",
    "对于这个秘籍，我们将只使用 torch 和 torchvision 来访问数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac42e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 744 ms (started: 2021-09-05 15:30:07 +08:00)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719592df",
   "metadata": {},
   "source": [
    "2. 加载和规范化数据集\n",
    "\n",
    "PyTorch 具有各种内置数据集（有关更多信息，请参阅加载数据配方）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05b4a888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/cifar-10-python.tar.gz\n",
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "time: 3.11 s (started: 2021-09-05 18:54:36 +08:00)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9332281b",
   "metadata": {},
   "source": [
    "3. 构建神经网络\n",
    "\n",
    "我们将使用卷积神经网络。 要了解更多信息，请参阅定义神经网络食谱。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "203e57b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.63 ms (started: 2021-09-05 18:55:16 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330151c7",
   "metadata": {},
   "source": [
    "4. 定义一个损失函数和优化器\n",
    "\n",
    "让我们使用带有动量的分类交叉熵损失和 SGD。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "642b73da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28.1 ms (started: 2021-09-05 18:55:39 +08:00)\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c5efba",
   "metadata": {},
   "source": [
    "5. 在训练网络时将梯度归零\n",
    "\n",
    "这是事情开始变得有趣的时候。 我们只需要遍历我们的数据迭代器，并将输入提供给网络并进行优化。\n",
    "\n",
    "请注意，对于每个数据实体，我们将梯度归零。 这是为了确保我们在训练神经网络时不会跟踪任何不必要的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ad391c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/miniconda3/envs/torch19/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.195\n",
      "[1,  4000] loss: 1.853\n",
      "[1,  6000] loss: 1.677\n",
      "[1,  8000] loss: 1.586\n",
      "[1, 10000] loss: 1.503\n",
      "[1, 12000] loss: 1.462\n",
      "[2,  2000] loss: 1.359\n",
      "[2,  4000] loss: 1.355\n",
      "[2,  6000] loss: 1.330\n",
      "[2,  8000] loss: 1.317\n",
      "[2, 10000] loss: 1.282\n",
      "[2, 12000] loss: 1.265\n",
      "Finished Training\n",
      "time: 1min 56s (started: 2021-09-05 18:57:35 +08:00)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c894475",
   "metadata": {},
   "source": [
    "您也可以使用 model.zero_grad()。 只要您的所有模型参数都在该优化器中，这与使用 optimizer.zero_grad() 相同。 使用您的最佳判断来决定使用哪一个。\n",
    "\n",
    "恭喜！ 您已成功将 PyTorch 的梯度归零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fe805b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch19]",
   "language": "python",
   "name": "conda-env-torch19-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
