{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3649185c",
   "metadata": {},
   "source": [
    "Captum 帮助您了解数据特征如何影响您的模型预测或神经元激活，揭示您的模型如何运作。\n",
    "\n",
    "使用 Captum，您可以统一应用各种最先进的特征归因算法，例如 Guided GradCam 和 Integrated Gradients。\n",
    "\n",
    "在这个秘籍中，您将学习如何使用 Captum 来： \n",
    "* 将图像分类器的预测归因于它们相应的图像特征。 \n",
    "* 可视化归因结果。\n",
    "\n",
    "# 在你开始之前\n",
    "\n",
    "确保 Captum 安装在活动的 Python 环境中。 Captum 可在 GitHub 上以 pip 包或 conda 包的形式使用。 有关详细说明，请参阅 https://captum.ai/ 上的安装指南\n",
    "\n",
    "对于模型，我们使用 PyTorch 中的内置图像分类器。 Captum 可以揭示样本图像的哪些部分支持模型做出的某些预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8537e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True).eval()\n",
    "\n",
    "response = requests.get(\"https://image.freepik.com/free-photo/two-beautiful-puppies-cat-dog_58409-6024.jpg\")\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "center_crop = transforms.Compose([\n",
    " transforms.Resize(256),\n",
    " transforms.CenterCrop(224),\n",
    "])\n",
    "\n",
    "normalize = transforms.Compose([\n",
    "    transforms.ToTensor(),               # converts the image to a tensor with values between 0 and 1\n",
    "    transforms.Normalize(                # normalize to follow 0-centered imagenet pixel rgb distribution\n",
    "     mean=[0.485, 0.456, 0.406],\n",
    "     std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "input_img = normalize(center_crop(img)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93941b5",
   "metadata": {},
   "source": [
    "# 计算归因\n",
    "\n",
    "在模型的前 3 个预测中是 208 和 283 类，它们对应于狗和猫。\n",
    "\n",
    "让我们使用 Captum 的遮挡算法将这些预测中的每一个归因于输入的相应部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44a68a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import Occlusion\n",
    "\n",
    "occlusion = Occlusion(model)\n",
    "\n",
    "strides = (3, 9, 9)               # smaller = more fine-grained attribution but slower\n",
    "target=208,                       # Labrador index in ImageNet\n",
    "sliding_window_shapes=(3,45, 45)  # choose size enough to change object appearance\n",
    "baselines = 0                     # values to occlude the image with. 0 corresponds to gray\n",
    "\n",
    "attribution_dog = occlusion.attribute(input_img,\n",
    "                                       strides = strides,\n",
    "                                       target=target,\n",
    "                                       sliding_window_shapes=sliding_window_shapes,\n",
    "                                       baselines=baselines)\n",
    "\n",
    "\n",
    "target=283,                       # Persian cat index in ImageNet\n",
    "attribution_cat = occlusion.attribute(input_img,\n",
    "                                       strides = strides,\n",
    "                                       target=target,\n",
    "                                       sliding_window_shapes=sliding_window_shapes,\n",
    "                                       baselines=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c818009",
   "metadata": {},
   "source": [
    "除了遮挡之外，Captum 还具有许多算法，例如集成梯度、反卷积、GuidedBackprop、GradCam、DeepLift 和 GradientShap。 所有这些算法都是 Attribution 的子类，它期望您的模型在初始化时作为可调用的 forward_func 并具有以统一格式返回归因结果的 attribute(...) 方法。\n",
    "\n",
    "让我们对图像的计算归因结果进行可视化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0793e3",
   "metadata": {},
   "source": [
    "# 可视化结果\n",
    "\n",
    "Captum 的可视化实用程序提供了开箱即用的方法来可视化图片和文本输入的归因结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b689ce77",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3a97bfef79d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Convert the compute attribution tensor into an image-like numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mattribution_dog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattribution_dog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvis_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"heat_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"original_image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "# Convert the compute attribution tensor into an image-like numpy array\n",
    "attribution_dog = np.transpose(attribution_dog.squeeze().cpu().detach().numpy(), (1,2,0))\n",
    "\n",
    "vis_types = [\"heat_map\", \"original_image\"]\n",
    "vis_signs = [\"all\", \"all\"] # \"positive\", \"negative\", or \"all\" to show both\n",
    "# positive attribution indicates that the presence of the area increases the prediction score\n",
    "# negative attribution indicates distractor areas whose absence increases the score\n",
    "\n",
    "_ = viz.visualize_image_attr_multiple(attribution_dog,\n",
    "                                      center_crop(img),\n",
    "                                      vis_types,\n",
    "                                      vis_signs,\n",
    "                                      [\"attribution for dog\", \"image\"],\n",
    "                                      show_colorbar = True\n",
    "                                     )\n",
    "\n",
    "\n",
    "attribution_cat = np.transpose(attribution_cat.squeeze().cpu().detach().numpy(), (1,2,0))\n",
    "\n",
    "_ = viz.visualize_image_attr_multiple(attribution_cat,\n",
    "                                      center_crop(img),\n",
    "                                      [\"heat_map\", \"original_image\"],\n",
    "                                      [\"all\", \"all\"], # positive/negative attribution or all\n",
    "                                      [\"attribution for cat\", \"image\"],\n",
    "                                      show_colorbar = True\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e218ff23",
   "metadata": {},
   "source": [
    "如果您的数据是文本数据，visualize.visualize_text() 提供了一个专门的视图来探索输入文本之上的属性。 在 http://captum.ai/tutorials/IMDB_TorchText_Interpret 了解更多信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7009baba",
   "metadata": {},
   "source": [
    "# 最后的注意点\n",
    "\n",
    "Captum 可以处理 PyTorch 中的大多数模型类型，包括视觉、文本等。 使用 Captum，您可以： \n",
    "* 将特定输出归因于模型输入，如上所示。 \n",
    "* 将特定输出归因于隐藏层神经元（请参阅 Captum API 参考）。 \n",
    "* 将隐藏层神经元响应归因于模型输入（参见 Captum API 参考）。\n",
    "\n",
    "有关支持方法的完整 API 和教程列表，请访问我们的网站 http://captum.ai\n",
    "\n",
    "Gilbert Tanner 的另一篇有用的帖子：https://gilberttanner.com/blog/interpreting-pytorch-models-with-captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f897e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch19]",
   "language": "python",
   "name": "conda-env-torch19-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
