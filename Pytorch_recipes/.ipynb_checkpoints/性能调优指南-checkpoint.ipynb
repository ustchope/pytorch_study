{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936be980",
   "metadata": {},
   "source": [
    "Performance Tuning Guide 是一组优化和最佳实践，可以加速 PyTorch 中深度学习模型的训练和推理。 提出的技术通常只需更改几行代码即可实现，并且可以应用于所有领域的各种深度学习模型。\n",
    "\n",
    "# 一般优化\n",
    "\n",
    "## 启用异步数据加载和扩充"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9934ab40",
   "metadata": {},
   "source": [
    "torch.utils.data.DataLoader 在单独的工作子进程中支持异步数据加载和数据扩充。 DataLoader 的默认设置是 num_workers=0，这意味着数据加载是同步的，并且在主进程中完成。 因此，主训练过程必须等待数据可用才能继续执行。\n",
    "\n",
    "设置 num_workers > 0 启用异步数据加载以及训练和数据加载之间的重叠。 num_workers 应该根据工作负载、CPU、GPU 和训练数据的位置进行调整。\n",
    "\n",
    "DataLoader 接受 pin_memory 参数，默认为 False。 使用 GPU 时，最好设置 pin_memory=True，这会指示 DataLoader 使用固定内存并启用从主机到 GPU 的更快的异步内存复制。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc92f3e2",
   "metadata": {},
   "source": [
    "## 禁用梯度计算以进行验证或推理\n",
    "\n",
    "PyTorch 从所有涉及需要梯度的张量的操作中保存中间缓冲区。通常，验证或推理不需要梯度。 torch.no_grad() 上下文管理器可用于禁用指定代码块内的梯度计算，这可以加速执行并减少所需的内存量。 torch.no_grad() 也可以用作函数装饰器。\n",
    "\n",
    "## 禁用卷积的偏差，直接跟随一个BN\n",
    "\n",
    "torch.nn.Conv2d() 具有默认为 True 的偏差参数（对于 Conv1d 和 Conv3d 也是如此）。\n",
    "\n",
    "如果 nn.Conv2d 层直接跟在 nn.BatchNorm2d 层之后，则不需要卷积中的偏差，而是使用 nn.Conv2d(..., bias=False, ....)。不需要偏差，因为在第一步 BatchNorm 减去平均值，这有效地抵消了偏差的影响。\n",
    "\n",
    "这也适用于 1d 和 3d 卷积，只要 BatchNorm（或其他归一化层）在与卷积偏差相同的维度上归一化。\n",
    "\n",
    "torchvision 提供的模型已经实现了这种优化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e718e13",
   "metadata": {},
   "source": [
    "## 使用 `parameter.grad = None` 而不是 `model.zero_grad()` 或 `optimizer.zero_grad()`\n",
    "\n",
    "而不是调用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b1a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "# or\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ca966",
   "metadata": {},
   "source": [
    "要将梯度归零，请改用以下方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670302cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff7ab7",
   "metadata": {},
   "source": [
    "第二个代码片段没有将每个单独参数的内存归零，后续的反向传递也使用赋值而不是加法来存储梯度，这减少了内存操作的数量。\n",
    "\n",
    "将梯度设置为无与将其设置为零的数值行为略有不同，有关更多详细信息，请参阅文档。\n",
    "\n",
    "或者，从 PyTorch 1.7 开始，调用 `model` 或 `optimizer.zero_grad(set_to_none=True)`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98fa441",
   "metadata": {},
   "source": [
    "## 融合逐点操作\n",
    "\n",
    "逐点运算（按元素加法、乘法、数学函数 - sin()、cos()、sigmoid() 等）可以融合到单个内核中，以分摊内存访问时间和内核启动时间。\n",
    "\n",
    "PyTorch JIT 可以自动融合内核，尽管编译器中可能还没有实现其他融合机会，并且并非所有设备类型都得到同等支持。\n",
    "\n",
    "Pointwise 操作受内存限制，对于每个操作 PyTorch 都会启动一个单独的内核。 每个内核从内存加载数据，执行计算（这一步通常很便宜）并将结果存储回内存。\n",
    "\n",
    "Fused operator 仅为多个融合 pointwise ops 启动一个内核，并且仅将数据加载/存储一次到内存。 这使得 JIT 对于激活函数、优化器、自定义 RNN 单元等非常有用。\n",
    "\n",
    "在最简单的情况下，可以通过将 torch.jit.script 装饰器应用于函数定义来启用融合，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d40210",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def fused_gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe43a98",
   "metadata": {},
   "source": [
    "有关更高级的用例，请参阅 TorchScript 文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e7d1b",
   "metadata": {},
   "source": [
    "## 为计算机视觉模型启用 channels_last 内存格式\n",
    "\n",
    "PyTorch 1.5 引入了对卷积网络的 channels_last 内存格式的支持。 这种格式旨在与 AMP 结合使用，以进一步加速具有 Tensor Cores 的卷积神经网络。\n",
    "\n",
    "对 channels_last 的支持是实验性的，但预计它适用于标准计算机视觉模型（例如 ResNet-50、SSD）。 要将模型转换为 channels_last 格式，请遵循 Channels Last Memory Format Tutorial。 本教程包括一个关于转换现有模型的部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d97076",
   "metadata": {},
   "source": [
    "## 检查点中间缓冲区\n",
    "\n",
    "缓冲区检查点是一种减轻模型训练内存容量负担的技术。 它不是存储所有层的输入来计算反向传播中的上游梯度，而是存储几个层的输入，其他层在反向传播期间重新计算。 减少的内存需求可以增加批次大小，从而提高利用率。\n",
    "\n",
    "应仔细选择检查点目标。 最好不要存储具有小的重新计算成本的大层输出。 示例目标层是激活函数（例如 ReLU、Sigmoid、Tanh）、上/下采样和具有小累积深度的矩阵向量操作。\n",
    "\n",
    "PyTorch 支持原生的 torch.utils.checkpoint API 来自动执行检查点和重新计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd27d75",
   "metadata": {},
   "source": [
    "## 禁用调试 API\n",
    "\n",
    "许多 PyTorch API 用于调试，应该在常规训练运行时禁用：\n",
    "* 异常检测：torch.autograd.detect_anomaly 或 torch.autograd.set_detect_anomaly(True)\n",
    "* 分析器相关：torch.autograd.profiler.emit_nvtx、torch.autograd.profiler.profile\n",
    "* autograd gradcheck：torch.autograd.gradcheck 或 torch.autograd.gradgradcheck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfee37b",
   "metadata": {},
   "source": [
    "# CPU 特定的优化\n",
    "\n",
    "## 利用非统一内存访问 (NUMA) 控件\n",
    "\n",
    "NUMA 或非均匀内存访问是一种用于数据中心机器的内存布局设计，旨在利用具有多个内存控制器和块的多插槽机器中的内存局部性。 一般来说，所有深度学习工作负载、训练或推理，无需跨 NUMA 节点访问硬件资源即可获得更好的性能。 因此，推理可以使用多个实例运行，每个实例运行在一个套接字上，以提高吞吐量。 对于单节点的训练任务，建议分布式训练，使每个训练过程都运行在一个socket上。\n",
    "\n",
    "在一般情况下，以下命令仅在第 N 个节点上的内核上执行 PyTorch 脚本，并避免跨套接字内存访问以减少内存访问开销。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a316a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numactl --cpunodebind=N --membind=N python <pytorch_script>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d613d1",
   "metadata": {},
   "source": [
    "## 使用 OpenMP\n",
    "\n",
    "OpenMP 用于为并行计算任务带来更好的性能。 OMP_NUM_THREADS 是可用于加速计算的最简单的开关。 它确定用于 OpenMP 计算的线程数。 CPU 亲和性设置控制工作负载如何分布在多个内核上。 它会影响通信开销、缓存行无效开销或页面抖动，因此正确设置 CPU 亲和性会带来性能优势。 GOMP_CPU_AFFINITY 或 KMP_AFFINITY 决定如何将 OpenMP* 线程绑定到物理处理单元。 详细信息可以在这里找到。\n",
    "\n",
    "使用以下命令，PyTorch 在 N 个 OpenMP 线程上运行任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c836c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export OMP_NUM_THREADS=N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7175bb",
   "metadata": {},
   "source": [
    "通常，以下环境变量用于设置与 GNU OpenMP 实现的 CPU 亲和性。 OMP_PROC_BIND 指定线程是否可以在处理器之间移动。 将其设置为 CLOSE 可使 OpenMP 线程靠近连续位置分区中的主线程。 OMP_SCHEDULE 确定如何调度 OpenMP 线程。 GOMP_CPU_AFFINITY 将线程绑定到特定的 CPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ca0f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export OMP_SCHEDULE=STATIC\n",
    "# export OMP_PROC_BIND=CLOSE\n",
    "# export GOMP_CPU_AFFINITY=\"N-M\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08449b56",
   "metadata": {},
   "source": [
    "## 英特尔 OpenMP 运行时库 (libiomp)\n",
    "\n",
    "默认情况下，PyTorch 使用 GNU OpenMP (GNU libgomp) 进行并行计算。 在英特尔平台上，英特尔 OpenMP 运行时库 (libiomp) 提供 OpenMP API 规范支持。 与 libgomp 相比，它有时会带来更多的性能优势。 利用环境变量 LD_PRELOAD 可以将 OpenMP 库切换到 libiomp："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ad985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export LD_PRELOAD=<path>/libiomp5.so:$LD_PRELOAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77cd682",
   "metadata": {},
   "source": [
    "与 GNU OpenMP 中的 CPU 关联设置类似，libiomp 中提供了环境变量来控制 CPU 关联设置。 KMP_AFFINITY 将 OpenMP 线程绑定到物理处理单元。 KMP_BLOCKTIME 设置线程在完成并行区域的执行后，在休眠之前应等待的时间（以毫秒为单位）。 在大多数情况下，将 KMP_BLOCKTIME 设置为 1 或 0 会产生良好的性能。 以下命令显示了英特尔 OpenMP 运行时库的常见设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805f7d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export KMP_AFFINITY=granularity=fine,compact,1,0\n",
    "# export KMP_BLOCKTIME=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb245ed1",
   "metadata": {},
   "source": [
    "## 切换内存分配器\n",
    "\n",
    "对于深度学习工作负载，与默认的 malloc 函数相比，Jemalloc 或 TCMalloc 可以通过尽可能多地重用内存来获得更好的性能。 Jemalloc 是一个通用的 malloc 实现，它强调避免碎片和可扩展的并发支持。 TCMalloc 还提供了一些优化来加速程序执行。 其中之一是将内存保存在缓存中以加快对常用对象的访问。 如果以后重新分配此类内存，即使在释放后保留此类缓存也有助于避免代价高昂的系统调用。 使用环境变量 LD_PRELOAD 来利用其中之一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d31ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export LD_PRELOAD=<jemalloc.so/tcmalloc.so>:$LD_PRELOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e72d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch19]",
   "language": "python",
   "name": "conda-env-torch19-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
