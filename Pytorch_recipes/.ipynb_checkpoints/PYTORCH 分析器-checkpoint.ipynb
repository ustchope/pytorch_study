{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c8fbf7",
   "metadata": {},
   "source": [
    "这个秘籍解释了如何使用 PyTorch 分析器并测量模型操作符的时间和内存消耗。\n",
    "\n",
    "# 介绍\n",
    "\n",
    "PyTorch 包含一个简单的分析器 API，当用户需要确定模型中最昂贵的运算符时，该 API 非常有用。\n",
    "\n",
    "在这个秘籍中，我们将使用一个简单的 Resnet 模型来演示如何使用 Profiler 来分析模型性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2939a8a6",
   "metadata": {},
   "source": [
    "步骤\n",
    "1. 导入所有必要的库\n",
    "2. 实例化一个简单的 Resnet 模型\n",
    "3. 使用探查器分析执行时间\n",
    "4. 使用探查器分析内存消耗\n",
    "5. 使用跟踪功能\n",
    "6. 检查堆栈跟踪\n",
    "7. 将数据可视化为火焰图\n",
    "8. 使用探查器分析长时间运行的作业  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb428b",
   "metadata": {},
   "source": [
    "1. 导入所有必要的库\n",
    "\n",
    "在这个秘籍中，我们将使用 torch、torchvision.models 和 profiler 模块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b44bc08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2180b681",
   "metadata": {},
   "source": [
    "2. 实例化一个简单的 Resnet 模型\n",
    "\n",
    "让我们创建一个 Resnet 模型的实例并为其准备输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83c52a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18()\n",
    "inputs = torch.randn(5, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012079fc",
   "metadata": {},
   "source": [
    "3.使用profiler分析执行时间\n",
    "\n",
    "PyTorch 分析器通过上下文管理器启用并接受许多参数，其中一些最有用的是：\n",
    "\n",
    "* 活动 - 要分析的活动列表：\n",
    "    * ProfilerActivity.CPU - PyTorch 操作符、TorchScript 函数和用户定义的代码标签（参见下面的 record_function）；\n",
    "    * ProfilerActivity.CUDA - 设备上的 CUDA 内核；\n",
    "* record_shapes - 是否记录操作员输入的形状；\n",
    "* profile_memory - 是否报告模型张量消耗的内存量；\n",
    "* use_cuda - 是否测量 CUDA 内核的执行时间。\n",
    "\n",
    "注意：使用 CUDA 时，分析器还会显示主机上发生的运行时 CUDA 事件。\n",
    "\n",
    "让我们看看我们如何使用分析器来分析执行时间："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30c8d61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/miniconda3/envs/torch19/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf780073",
   "metadata": {},
   "source": [
    "请注意，我们可以使用 record_function 上下文管理器用用户提供的名称标记任意代码范围（model_inference 在上面的示例中用作标签）。\n",
    "\n",
    "Profiler 允许人们检查在使用 Profiler 上下文管理器包装的代码范围的执行期间调用了哪些运算符。 如果多个分析器范围同时处于活动状态（例如在并行 PyTorch 线程中），则每个分析上下文管理器仅跟踪其相应范围的运算符。 Profiler 还会自动分析使用 torch.jit._fork 启动的异步任务和（在向后传递的情况下）使用向后（）调用启动的向后传递运算符。\n",
    "\n",
    "让我们打印出上面执行的统计信息："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "267dc145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  model_inference         7.85%       8.701ms        99.04%     109.710ms     109.710ms             1  \n",
      "                     aten::conv2d         0.21%     233.000us        66.13%      73.255ms       3.663ms            20  \n",
      "                aten::convolution         0.26%     286.000us        65.92%      73.022ms       3.651ms            20  \n",
      "               aten::_convolution         0.82%     905.000us        65.66%      72.736ms       3.637ms            20  \n",
      "         aten::mkldnn_convolution        64.41%      71.350ms        64.84%      71.831ms       3.592ms            20  \n",
      "                 aten::batch_norm         0.20%     224.000us        17.76%      19.678ms     983.900us            20  \n",
      "     aten::_batch_norm_impl_index         0.33%     367.000us        17.56%      19.454ms     972.700us            20  \n",
      "          aten::native_batch_norm         9.61%      10.650ms        17.16%      19.013ms     950.650us            20  \n",
      "                       aten::mean         0.42%     465.000us         5.04%       5.582ms     265.810us            21  \n",
      "                 aten::max_pool2d         0.04%      40.000us         3.43%       3.804ms       3.804ms             1  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 110.775ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ad789",
   "metadata": {},
   "source": [
    "在这里我们看到，正如预期的那样，大部分时间都花在了卷积上（特别是在使用 MKL-DNN 支持编译的 PyTorch 的 mkldnn_convolution 中）。 注意self cpu time和cpu time的区别——运营商可以调用其他运营商，self cpu time不包括子运营商调用花费的时间，而总cpu time包括它。 您可以通过将 sort_by=\"self_cpu_time_total\" 传递到 table 调用中来选择按 self cpu 时间排序。\n",
    "\n",
    "要获得更精细的结果粒度并包含操作员输入形状，请传递 group_by_input_shape=True（注意：这需要使用 record_shapes=True 运行分析器）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4424c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls                                                                      Input Shapes  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "                  model_inference         7.85%       8.701ms        99.04%     109.710ms     109.710ms             1                                                                                []  \n",
      "                     aten::conv2d         0.05%      51.000us        14.90%      16.506ms       4.127ms             4                             [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []]  \n",
      "                aten::convolution         0.05%      58.000us        14.85%      16.455ms       4.114ms             4                     [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], []]  \n",
      "               aten::_convolution         0.10%     114.000us        14.80%      16.397ms       4.099ms             4     [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], [], [], [], [], [], [], []]  \n",
      "         aten::mkldnn_convolution        14.58%      16.147ms        14.70%      16.283ms       4.071ms             4                             [[5, 64, 56, 56], [64, 64, 3, 3], [], [], [], [], []]  \n",
      "                     aten::conv2d         0.03%      37.000us        11.52%      12.764ms       4.255ms             3                          [[5, 256, 14, 14], [256, 256, 3, 3], [], [], [], [], []]  \n",
      "                aten::convolution         0.04%      40.000us        11.49%      12.727ms       4.242ms             3                  [[5, 256, 14, 14], [256, 256, 3, 3], [], [], [], [], [], [], []]  \n",
      "               aten::_convolution         0.06%      62.000us        11.45%      12.687ms       4.229ms             3  [[5, 256, 14, 14], [256, 256, 3, 3], [], [], [], [], [], [], [], [], [], [], []]  \n",
      "         aten::mkldnn_convolution        11.35%      12.570ms        11.40%      12.625ms       4.208ms             3                          [[5, 256, 14, 14], [256, 256, 3, 3], [], [], [], [], []]  \n",
      "                     aten::conv2d         0.02%      25.000us        10.03%      11.108ms      11.108ms             1                             [[5, 3, 224, 224], [64, 3, 7, 7], [], [], [], [], []]  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  --------------------------------------------------------------------------------  \n",
      "Self CPU time total: 110.775ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666607d3",
   "metadata": {},
   "source": [
    "注意 aten::convolution 两次出现，输入形状不同。  \n",
    "Profiler 还可用于分析在 GPU 上执行的模型的性能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba05ecd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference         4.88%       2.751ms        97.86%      55.127ms      55.127ms       0.000us         0.00%       1.665ms       1.665ms             1  \n",
      "                                           aten::conv2d         0.25%     139.000us        75.47%      42.512ms       2.126ms       0.000us         0.00%       1.231ms      61.550us            20  \n",
      "                                      aten::convolution         0.26%     144.000us        75.22%      42.373ms       2.119ms       0.000us         0.00%       1.231ms      61.550us            20  \n",
      "                                     aten::_convolution         0.42%     234.000us        74.97%      42.229ms       2.111ms       0.000us         0.00%       1.231ms      61.550us            20  \n",
      "                                aten::cudnn_convolution        50.25%      28.308ms        74.55%      41.995ms       2.100ms       1.231ms        73.93%       1.231ms      61.550us            20  \n",
      "ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile14...         0.00%       0.000us         0.00%       0.000us       0.000us     283.000us        17.00%     283.000us      47.167us             6  \n",
      "                                       aten::batch_norm         0.25%     143.000us         7.61%       4.287ms     214.350us       0.000us         0.00%     259.000us      12.950us            20  \n",
      "                           aten::_batch_norm_impl_index         0.32%     179.000us         7.36%       4.144ms     207.200us       0.000us         0.00%     259.000us      12.950us            20  \n",
      "                                 aten::cudnn_batch_norm         1.95%       1.100ms         7.04%       3.965ms     198.250us     259.000us        15.56%     259.000us      12.950us            20  \n",
      "ampere_scudnn_winograd_128x128_ldg1_ldg4_relu_tile14...         0.00%       0.000us         0.00%       0.000us       0.000us     214.000us        12.85%     214.000us      53.500us             4  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 56.331ms\n",
      "Self CUDA time total: 1.665ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18().cuda()\n",
    "inputs = torch.randn(5, 3, 224, 224).cuda()\n",
    "\n",
    "with profile(activities=[\n",
    "        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f2ff6d",
   "metadata": {},
   "source": [
    "请注意输出中出现的设备上内核（例如 sgemm_32x32x32 NN）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef382f9",
   "metadata": {},
   "source": [
    "4. 使用profiler分析内存消耗\n",
    "\n",
    "PyTorch 分析器还可以显示在模型运算符执行期间分配（或释放）的内存量（由模型的张量使用）。 在下面的输出中，“self”内存对应于操作符分配（释放）的内存，不包括对其他操作符的子调用。 要启用内存分析功能，请传递 profile_memory=True。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c8c3d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                      aten::empty         0.85%     773.000us         0.85%     773.000us       7.653us      94.79 Mb      94.79 Mb           101  \n",
      "    aten::max_pool2d_with_indices         3.36%       3.049ms         3.36%       3.049ms       3.049ms      11.48 Mb      11.48 Mb             1  \n",
      "                      aten::addmm         0.35%     317.000us         0.37%     333.000us     333.000us      19.53 Kb      19.53 Kb             1  \n",
      "                        aten::add         0.31%     281.000us         0.31%     281.000us      14.050us         160 b         160 b            20  \n",
      "              aten::empty_strided         0.26%     236.000us         0.26%     236.000us      11.238us          84 b          84 b            21  \n",
      "                     aten::conv2d         0.17%     151.000us        70.07%      63.657ms       3.183ms      47.37 Mb           0 b            20  \n",
      "                aten::convolution         0.18%     159.000us        69.91%      63.506ms       3.175ms      47.37 Mb           0 b            20  \n",
      "               aten::_convolution         0.29%     262.000us        69.73%      63.347ms       3.167ms      47.37 Mb           0 b            20  \n",
      "         aten::mkldnn_convolution        69.13%      62.799ms        69.44%      63.085ms       3.154ms      47.37 Mb           0 b            20  \n",
      "                aten::as_strided_         0.07%      65.000us         0.07%      65.000us       3.250us           0 b           0 b            20  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 90.845ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18()\n",
    "inputs = torch.randn(5, 3, 224, 224)\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU],\n",
    "        profile_memory=True, record_shapes=True) as prof:\n",
    "    model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7998a215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                      aten::empty         0.85%     773.000us         0.85%     773.000us       7.653us      94.79 Mb      94.79 Mb           101  \n",
      "                 aten::batch_norm         0.16%     145.000us        21.69%      19.701ms     985.050us      47.41 Mb           0 b            20  \n",
      "     aten::_batch_norm_impl_index         0.25%     225.000us        21.53%      19.556ms     977.800us      47.41 Mb           0 b            20  \n",
      "          aten::native_batch_norm        12.45%      11.312ms        21.22%      19.277ms     963.850us      47.41 Mb           0 b            20  \n",
      "                     aten::conv2d         0.17%     151.000us        70.07%      63.657ms       3.183ms      47.37 Mb           0 b            20  \n",
      "                aten::convolution         0.18%     159.000us        69.91%      63.506ms       3.175ms      47.37 Mb           0 b            20  \n",
      "               aten::_convolution         0.29%     262.000us        69.73%      63.347ms       3.167ms      47.37 Mb           0 b            20  \n",
      "         aten::mkldnn_convolution        69.13%      62.799ms        69.44%      63.085ms       3.154ms      47.37 Mb           0 b            20  \n",
      "                 aten::max_pool2d         0.01%      12.000us         3.37%       3.061ms       3.061ms      11.48 Mb           0 b             1  \n",
      "    aten::max_pool2d_with_indices         3.36%       3.049ms         3.36%       3.049ms       3.049ms      11.48 Mb      11.48 Mb             1  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 90.845ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321942b7",
   "metadata": {},
   "source": [
    "5. 使用追踪功能\n",
    "\n",
    "分析结果可以作为 .json 跟踪文件输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dda8bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18().cuda()\n",
    "inputs = torch.randn(5, 3, 224, 224).cuda()\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "    model(inputs)\n",
    "\n",
    "prof.export_chrome_trace(\"trace.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621d82cd",
   "metadata": {},
   "source": [
    "6. 检查堆栈跟踪\n",
    "\n",
    "Profiler 可用于分析 Python 和 TorchScript 堆栈跟踪："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9291784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Source Location                                                              \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  \n",
      "                                aten::cudnn_convolution         4.47%       2.209ms         9.21%       4.546ms     568.250us     596.000us        33.50%     596.000us      74.500us             8  ...lib/python3.9/site-packages/torch/nn/modules/conv.py(439): _conv_forward  \n",
      "                                                                                                                                                                                                     ...rch19/lib/python3.9/site-packages/torch/nn/modules/conv.py(443): forward  \n",
      "                                                                                                                                                                                                     ...lib/python3.9/site-packages/torch/nn/modules/module.py(1051): _call_impl  \n",
      "                                                                                                                                                                                                     ...19/lib/python3.9/site-packages/torchvision/models/resnet.py(70): forward  \n",
      "                                                                                                                                                                                                     ...lib/python3.9/site-packages/torch/nn/modules/module.py(1051): _call_impl  \n",
      "                                                                                                                                                                                                                                                                                  \n",
      "                                aten::cudnn_convolution         3.46%       1.707ms         7.72%       3.814ms     476.750us     555.000us        31.20%     555.000us      69.375us             8  ...lib/python3.9/site-packages/torch/nn/modules/conv.py(439): _conv_forward  \n",
      "                                                                                                                                                                                                     ...rch19/lib/python3.9/site-packages/torch/nn/modules/conv.py(443): forward  \n",
      "                                                                                                                                                                                                     ...lib/python3.9/site-packages/torch/nn/modules/module.py(1051): _call_impl  \n",
      "                                                                                                                                                                                                     ...19/lib/python3.9/site-packages/torchvision/models/resnet.py(74): forward  \n",
      "                                                                                                                                                                                                     ...lib/python3.9/site-packages/torch/nn/modules/module.py(1051): _call_impl  \n",
      "                                                                                                                                                                                                                                                                                  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ---------------------------------------------------------------------------  \n",
      "Self CPU time total: 49.373ms\n",
      "Self CUDA time total: 1.779ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with profile(\n",
    "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    with_stack=True,\n",
    ") as prof:\n",
    "    model(inputs)\n",
    "\n",
    "# Print aggregated stats\n",
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9743cf",
   "metadata": {},
   "source": [
    "7. 将数据可视化为火焰图\n",
    "\n",
    "执行时间（self_cpu_time_total 和 self_cuda_time_total 指标）和堆栈跟踪也可以可视化为火焰图。 为此，首先使用 export_stacks 导出原始数据（需要 with_stack=True）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6d73d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.export_stacks(\"/tmp/profiler_stacks.txt\", \"self_cuda_time_total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b8a1e8",
   "metadata": {},
   "source": [
    "我们建议使用例如 生成交互式 SVG 的 Flamegraph 工具："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf0be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git clone https://github.com/brendangregg/FlameGraph\n",
    "# cd FlameGraph\n",
    "# ./flamegraph.pl --title \"CUDA time\" --countname \"us.\" /tmp/profiler_stacks.txt > perf_viz.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f86e826",
   "metadata": {},
   "source": [
    "8. 使用 profiler 分析长时间运行的作业\n",
    "\n",
    "PyTorch 分析器提供了一个额外的 API 来处理长时间运行的作业（例如训练循环）。 跟踪所有执行可能会很慢并导致非常大的跟踪文件。 为避免这种情况，请使用可选参数：\n",
    "\n",
    "* schedule - 指定一个将整数参数（步骤编号）作为输入并返回分析器操作的函数，使用此参数的最佳方法是使用可以为您生成时间表的 torch.profiler.schedule 辅助函数；\n",
    "* on_trace_ready - 指定一个函数，它将对探查器的引用作为输入，并在每次新跟踪准备好时由探查器调用。\n",
    "为了说明 API 的工作原理，让我们首先考虑以下带有 torch.profiler.schedule 辅助函数的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3acdb458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import schedule\n",
    "\n",
    "my_schedule = schedule(\n",
    "    skip_first=10,\n",
    "    wait=5,\n",
    "    warmup=1,\n",
    "    active=3,\n",
    "    repeat=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4680a36",
   "metadata": {},
   "source": [
    "Profiler 假定长时间运行的作业由步骤组成，从零开始编号。 上面的示例为探查器定义了以下操作序列：\n",
    "\n",
    "1. 参数skip_first 告诉profiler 它应该忽略前10 个步骤（skip_first 的默认值为零）；\n",
    "1. 在第一个 skip_first 步骤之后，探查器开始执行探查器周期；\n",
    "1. 每个周期由三个阶段组成：\n",
    "    * 空闲（等待 = 5 步），在此阶段分析器未激活；\n",
    "    * 预热（warmup=1 步），在此阶段分析器开始跟踪，但结果被丢弃； 此阶段用于丢弃在跟踪开始时分析器获得的样本，因为它们通常会因额外的开销而倾斜；\n",
    "    * 主动跟踪（主动=3 步），在此阶段分析器跟踪和记录数据；\n",
    "1. 可选的重复参数指定循环数的上限。 默认情况下（零值），只要作业运行，探查器就会执行循环。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb0ae80",
   "metadata": {},
   "source": [
    "因此，在上面的示例中，profiler 将跳过前 15 个步骤，将下一步花在热身上，主动记录接下来的 3 个步骤，再跳过 5 个步骤，将下一步花在热身上，再主动记录另外 3 个步骤 . 由于指定了 repeat=2 参数值，分析器将在前两个循环后停止记录。\n",
    "\n",
    "在每个周期结束时，分析器调用指定的 on_trace_ready 函数并将其自身作为参数传递。 此函数用于处理新的跟踪 - 通过获取表输出或将输出保存在磁盘上作为跟踪文件。\n",
    "\n",
    "要向分析器发送下一步已开始的信号，请调用 prof.step() 函数。 当前分析器步骤存储在 prof.step_num 中。\n",
    "\n",
    "以下示例显示了如何使用上述所有概念："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660fb565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch19]",
   "language": "python",
   "name": "conda-env-torch19-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
